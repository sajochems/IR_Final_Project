{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "C:\\Users\\waded\\AppData\\Local\\Temp\\ipykernel_35708\\178958555.py:6: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init()\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import tqdm\n",
    "import pyterrier as pt\n",
    "print(pt.__version__)\n",
    "\n",
    "pt.init()\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyterrier.measures import RR, nDCG, MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericDoc(doc_id='2020338_0', text=\"A small group of politicians believed strongly that the fact that Saddam Hussien remained in power after the first Gulf War was a signal of weakness to the rest of the world, one that invited attacks and terrorism. Shortly after taking power with George Bush in 2000 and after the attack on 9/11, they were able to use the terrorist attacks to justify war with Iraq on this basis and exaggerated threats of the development of weapons of mass destruction. The military strength of the U.S. and the brutality of Saddam's regime led them to imagine that the military and political victory would be relatively easy.\")\n"
     ]
    }
   ],
   "source": [
    "antique = ir_datasets.load(\"antique/test\")\n",
    "print(antique.docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:42:21.429 [main] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (730691_1) - further warnings are suppressed\n",
      "23:43:39.557 [main] WARN org.terrier.structures.indexing.Indexer -- Indexed 502 empty documents\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL - Index the data\n",
    "\n",
    "def antique_gen(limit=100000):\n",
    "    lastdoc = 0\n",
    "    for elem in antique.docs:\n",
    "        if lastdoc >= limit:\n",
    "            break\n",
    "        yield {\n",
    "            \"docno\": elem.doc_id,\n",
    "            \"text\": elem.text,\n",
    "        }\n",
    "        lastdoc += 1\n",
    "\n",
    "max(len(text.encode(\"utf-8\")) for _, text in antique.docs)\n",
    "idx_path = Path.cwd() / \"indices\" / \"antique_test\"\n",
    "indexer = pt.IterDictIndexer(\n",
    "    str(idx_path),\n",
    "    meta={\n",
    "        \"docno\": 20,\n",
    "        \"text\": 4096,\n",
    "    },\n",
    "    stemmer=\"porter\",\n",
    "    stopwords=\"terrier\",\n",
    ")\n",
    "\n",
    "index_ref = indexer.index(antique_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.direct.bf'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.document.fsarrayfile'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.inverted.bf'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.lexicon.fsomapfile'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.lexicon.fsomaphash'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.lexicon.fsomapid'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.meta-0.fsomapfile'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.meta.idx'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.meta.zdata'), WindowsPath('C:/Users/waded/OneDrive/Bureaublad/Information Retrieval/repo/indices/antique_test/data.properties')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\waded\\AppData\\Local\\Temp\\ipykernel_35708\\3402352017.py:11: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  retriever_antique = pt.BatchRetrieve(index_antique, wmodel=\"BM25\")\n"
     ]
    }
   ],
   "source": [
    "# Define index paths\n",
    "index_dir_antique = Path.cwd() / \"indices\" / \"antique_test\"\n",
    "print(index_dir_antique.exists())  # Should print True\n",
    "print(list(index_dir_antique.iterdir()))  # List the contents\n",
    "# Load the indexes\n",
    "#index_msmarco = pt.IndexFactory.of(str(index_dir_msmarco))\n",
    "index_antique = pt.IndexFactory.of(str(index_dir_antique))\n",
    "\n",
    "# Use BM25 as the baseline retriever\n",
    "#retriever_msmarco = pt.BatchRetrieve(index_msmarco, wmodel=\"BM25\")\n",
    "retriever_antique = pt.BatchRetrieve(index_antique, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert qrels to DataFrames\n",
    "#qrels_msmarco = pd.DataFrame(msmarco.qrels_iter())\n",
    "qrels_antique = pd.DataFrame(antique.qrels_iter())\n",
    "\n",
    "# Convert queries to DataFrames\n",
    "#queries_msmarco = pd.DataFrame(msmarco.queries_iter())\n",
    "queries_antique = pd.DataFrame(antique.queries_iter())\n",
    "\n",
    "# Rename columns for PyTerrier compatibility\n",
    "#qrels_msmarco.rename(columns={\"query_id\": \"qid\", \"doc_id\": \"docno\", \"relevance\": \"label\"}, inplace=True)\n",
    "qrels_antique.rename(columns={\"query_id\": \"qid\", \"doc_id\": \"docno\", \"relevance\": \"label\"}, inplace=True)\n",
    "\n",
    "#queries_msmarco.rename(columns={\"query_id\": \"qid\", \"text\": \"query\"}, inplace=True)\n",
    "queries_antique.rename(columns={\"query_id\": \"qid\", \"text\": \"query\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    query = query.encode(\"ascii\", \"ignore\").decode()\n",
    "    query = query.replace(\"'\", \"\").replace('\"', \"\").replace(\"`\", \"\")\n",
    "    query = re.sub(r\"\\s+\", \" \", query).strip()\n",
    "    return query\n",
    "\n",
    "#queries_msmarco[\"query\"] = queries_msmarco[\"query\"].apply(clean_query)\n",
    "queries_antique[\"query\"] = queries_antique[\"query\"].apply(clean_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model imports\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model\n",
    "MODEL_ID = \"prhegde/t5-query-reformulation-RL\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_ID)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to rewrite queries\n",
    "def rewrite_query(query, nsent=1):\n",
    "    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=35, num_beams=1, do_sample=True, repetition_penalty=1.8)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "queries_antique[\"rewritten_query\"] = queries_antique[\"query\"].apply(rewrite_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bm25_antique = pt.terrier.Retriever(index_antique, wmodel=\"BM25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: Failed to process qid 3990512 'how can we get concentration onsomething?' -- Lexical error at line 1, column 42.  Encountered: <EOF> after : \"\" org.terrier.querying.parser.QueryParserException",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mJavaException\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mpt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mExperiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mbm25_antique\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mqueries_antique\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrename\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrewritten_query\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquery\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Use rewritten queries\u001B[39;49;00m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mqrels_antique\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_metrics\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mRR\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnDCG\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMAP\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\pipelines.py:652\u001B[39m, in \u001B[36mExperiment\u001B[39m\u001B[34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, save_format, precompute_prefix, **kwargs)\u001B[39m\n\u001B[32m    649\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mUnrecognised save_mode \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % \u001B[38;5;28mstr\u001B[39m(save_format)) \n\u001B[32m    650\u001B[39m     save_file = os.path.join(save_dir, \u001B[33m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % (name, save_ext))\n\u001B[32m--> \u001B[39m\u001B[32m652\u001B[39m time, evalMeasuresDict = \u001B[43m_run_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m    \u001B[49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_topics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqrels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_metrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    654\u001B[39m \u001B[43m    \u001B[49m\u001B[43mperquery\u001B[49m\u001B[43m=\u001B[49m\u001B[43mperquery\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbaseline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    655\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    656\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbackfill_qids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_topic_qids\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mperquery\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    657\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    659\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_format\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    660\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    662\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m baseline \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    663\u001B[39m     evalDictsPerQ.append(evalMeasuresDict)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\pipelines.py:336\u001B[39m, in \u001B[36m_run_and_evaluate\u001B[39m\u001B[34m(system, topics, qrels, metrics, pbar, save_mode, save_file, save_format, perquery, batch_size, backfill_qids)\u001B[39m\n\u001B[32m    333\u001B[39m \u001B[38;5;66;03m#transformer, evaluate all queries at once\u001B[39;00m\n\u001B[32m    335\u001B[39m starttime = timer()\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m res = \u001B[43msystem\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtopics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    337\u001B[39m endtime = timer()\n\u001B[32m    338\u001B[39m runtime =  \u001B[38;5;28mfloat\u001B[39m(endtime - starttime) * \u001B[32m1000.\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\utils.py:208\u001B[39m, in \u001B[36mpre_invocation_decorator.<locals>._decorator_wrapper.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    205\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(fn)\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_wrapper\u001B[39m(*args, **kwargs):\n\u001B[32m    207\u001B[39m     decorator(fn)\n\u001B[32m--> \u001B[39m\u001B[32m208\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\terrier\\retriever.py:429\u001B[39m, in \u001B[36mRetriever.transform\u001B[39m\u001B[34m(self, queries)\u001B[39m\n\u001B[32m    427\u001B[39m         \u001B[38;5;28miter\u001B[39m = pt.tqdm(\u001B[38;5;28miter\u001B[39m, desc=\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m), total=queries.shape[\u001B[32m0\u001B[39m], unit=\u001B[33m\"\u001B[39m\u001B[33mq\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    428\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m         res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retrieve_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_results\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocno_provided\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdocno_provided\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocid_provided\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdocid_provided\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores_provided\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscores_provided\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    430\u001B[39m         results.extend(res)\n\u001B[32m    432\u001B[39m res_dt = pd.DataFrame(results, columns=[\u001B[33m'\u001B[39m\u001B[33mqid\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdocid\u001B[39m\u001B[33m'\u001B[39m ] + \u001B[38;5;28mself\u001B[39m.metadata + [\u001B[33m'\u001B[39m\u001B[33mrank\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\utils.py:208\u001B[39m, in \u001B[36mpre_invocation_decorator.<locals>._decorator_wrapper.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    205\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(fn)\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_wrapper\u001B[39m(*args, **kwargs):\n\u001B[32m    207\u001B[39m     decorator(fn)\n\u001B[32m--> \u001B[39m\u001B[32m208\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Bureaublad\\Information Retrieval\\repo\\venv\\Lib\\site-packages\\pyterrier\\terrier\\retriever.py:340\u001B[39m, in \u001B[36mRetriever._retrieve_one\u001B[39m\u001B[34m(self, row, input_results, docno_provided, docid_provided, scores_provided)\u001B[39m\n\u001B[32m    337\u001B[39m     srq.setControl(\u001B[33m\"\u001B[39m\u001B[33mmatching\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33morg.terrier.matching.ScoringMatching\u001B[39m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m + srq.getControl(\u001B[33m\"\u001B[39m\u001B[33mmatching\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m    339\u001B[39m \u001B[38;5;66;03m# now ask Terrier to run the request\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m340\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrunSearchRequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    341\u001B[39m result = srq.getResults()\n\u001B[32m    343\u001B[39m \u001B[38;5;66;03m# check we got all of the expected metadata (if the resultset has a size at all)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mjnius\\\\jnius_export_class.pxi:877\u001B[39m, in \u001B[36mjnius.JavaMethod.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mjnius\\\\jnius_export_class.pxi:971\u001B[39m, in \u001B[36mjnius.JavaMethod.call_method\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mjnius\\\\jnius_utils.pxi:79\u001B[39m, in \u001B[36mjnius.check_exception\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mJavaException\u001B[39m: JVM exception occurred: Failed to process qid 3990512 'how can we get concentration onsomething?' -- Lexical error at line 1, column 42.  Encountered: <EOF> after : \"\" org.terrier.querying.parser.QueryParserException"
     ]
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [bm25_antique],\n",
    "    queries_antique.rename(columns={\"rewritten_query\": \"query\"}),  # Use rewritten queries\n",
    "    qrels_antique,\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}